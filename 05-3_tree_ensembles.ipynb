{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217fc8b6",
   "metadata": {},
   "source": [
    "I want to see how tree ensembles behave on the wine dataset. I will try Random Forest and Extra Trees. Then I will move to boosting with Gradient Boosting and Histogram based Gradient Boosting. I will finish with XGBoost and LightGBM. I will keep notes inline so the reasoning is easy to follow later.\n",
    "\n",
    "The machine learning algorithms that I learned previously fits best on structured data. Among those algorithms, ensemble learning prints the best output for treating structured data, which is an algorithm is based on mostly decision tree. \n",
    "\n",
    "Now lets dig into the ultimate algorithm of treating structured data in scikit learn, the ensemble learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8682fe0",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07d293af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac8a7e1",
   "metadata": {},
   "source": [
    "# Load data and split\n",
    "\n",
    "I'll reuse the same wine set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17947700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>sugar</th>\n",
       "      <th>pH</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.8</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  sugar    pH  class\n",
       "0      9.4    1.9  3.51    0.0\n",
       "1      9.8    2.6  3.20    0.0\n",
       "2      9.8    2.3  3.26    0.0\n",
       "3      9.8    1.9  3.16    0.0\n",
       "4      9.4    1.9  3.51    0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine = pd.read_csv('https://bit.ly/wine_csv_data')\n",
    "\n",
    "X = wine[['alcohol', 'sugar', 'pH']]\n",
    "y = wine['class']\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b55159",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "**Random forest** is one of the representatives of ensemble learning and is widely used due to its stable per formance. As we can infer from the name itself, a random forst makes a forest of decision trees by randomizing decision trees, and we make a final prediction using the predictions of each decision tree. \n",
    "\n",
    "First, a random forest makes the data for training each tree random, which is unique in how it is created. It creates training data by randomly extracting samples from the training data we input. At this time, one sample may be duplicated and extracted.\n",
    "\n",
    "After all samples are extracted, it is called a **bootstrap sample**, and it is normally same as the size of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f191d3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9973541965122431 0.8903229806766861\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_jobs = -1, random_state = 42) #RandomForestClassifier uses 100 decision trees by default, so better use every CPU core by n_jobs = -1\n",
    "\n",
    "scores = cross_validate(rf, train_X, train_y,\n",
    "return_train_score = True, n_jobs = -1) #cross validation, return_train_score returns train score, better for finding out over/under fitting\n",
    "\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
    "\n",
    "#Seems like the set is overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01621e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23183515 0.50059756 0.26756729]\n"
     ]
    }
   ],
   "source": [
    "# Random forest is an ensemble of decision tree, so all the parameters that consists DecisionTreeClassifier are provided (also calculates feature importance).\n",
    "\n",
    "rf.fit(train_X, train_y)\n",
    "print(rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85c1a3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8945545507023283\n"
     ]
    }
   ],
   "source": [
    "# Another feature of RandomForestClassifier, it obtains the evaluation score of the model itself by OOB sample\n",
    "# OOB : Out of Bag - the samples that are not included in the bootstrap sample, works like a validation set\n",
    "\n",
    "rf = RandomForestClassifier (oob_score = True, n_jobs = -1, random_state = 42) # use oob_score = True to get the oob score\n",
    "rf.fit(train_X, train_y)\n",
    "\n",
    "print(rf.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3abde3",
   "metadata": {},
   "source": [
    "# Extra Trees\n",
    "\n",
    "Extra Trees uses more randomization at each split, which often gives similar accuracy with a different bias variance tradeoff.\n",
    "\n",
    "The extra tree uses the decision tree of DecisionTreeClassifier, with the parameter **splitter = 'random'** \n",
    "\n",
    "If you randomly split the features from one decision tree, it will overfit. However, ExtraTrees ensembles a number of trees so it prevents overfitting and increasing the score of the set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ea447d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9974503966084433 0.8887848893166506\n"
     ]
    }
   ],
   "source": [
    "et = ExtraTreesClassifier(n_jobs=-1, random_state=42)\n",
    "scores = cross_validate(et, train_X, train_y,\n",
    "return_train_score = True, n_jobs= - 1)\n",
    "\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcfd5be",
   "metadata": {},
   "source": [
    "The score is similar to the randomforest. Of course, since there are less samples to figure out the difference between the models. Normally, extra tree needs to train more trees since it have more randomness, but is also owns advantage in fast calculation speed since it divides the nodes randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20e9f603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20183568 0.52242907 0.27573525]\n"
     ]
    }
   ],
   "source": [
    "et.fit(train_X, train_y)\n",
    "\n",
    "print(et.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d51dccb",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "Gradient Boosting is a method of ensembleing by using shallow decision trees to compensate for errors in previous trees. By default, scikit learn's GradientEBoosting Classifier uses 100 decision trees with depths of 3.\n",
    "\n",
    "Because it uses shallow decision trees with depth, we can expect overfitting and generally high generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0117128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8881086892152563 0.8720430147331015\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(random_state = 42)\n",
    "scores = cross_validate(gb, train_X, train_y,\n",
    "return_train_score = True, n_jobs = -1)\n",
    "\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
    "\n",
    "#gradient boosting is resistant to overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fee1497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9464595437171814 0.8780082549788999\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators = 500, learning_rate = 0.2, random_state = 42) #default learning_rate = 0.1\n",
    "\n",
    "scores = cross_validate(gb, train_X, train_y,\n",
    "return_train_score = True, n_jobs = -1)\n",
    "\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76c77ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15881044 0.67988912 0.16130044]\n"
     ]
    }
   ],
   "source": [
    "gb.fit(train_X, train_y)\n",
    "\n",
    "print(gb.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234419c2",
   "metadata": {},
   "source": [
    "It still is resistant to overfitting even n_estimators have been increased five times. And also as you can see, gradient boosting foucses on specific feature than random forest. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e284c8fd",
   "metadata": {},
   "source": [
    "# Histogram-based Gradient Boosting\n",
    "\n",
    "Histogram-based Gradient Boosting is the most popular algorithm among those which treats structured data. \n",
    "\n",
    "Histogram-based Gradient Boosting divides input features into 256 sections, so it quickly finds the best metric while it divides the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b858bcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9321723946453317 0.8801241948619236\n"
     ]
    }
   ],
   "source": [
    "hgb = HistGradientBoostingClassifier(random_state = 42)\n",
    "scores = cross_validate(hgb, train_X, train_y,\n",
    "return_train_score = True)\n",
    "\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
    "\n",
    "#It controls overfitting and provides a better performance than gradient boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bf5f141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08876275 0.23438522 0.08027708]\n"
     ]
    }
   ],
   "source": [
    "# HistGradientBoostingClassifier doesn't provide feature_importances_, so we should use permutation_importance() to calculate it. \n",
    "\n",
    "hgb.fit(train_X, train_y)\n",
    "result = permutation_importance(hgb, train_X, train_y,\n",
    "n_repeats = 10 ,random_state = 42, n_jobs = -1)\n",
    "\n",
    "print(result.importances_mean)\n",
    "\n",
    "# result = [importances, importances_mean, importances_std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03622b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05969231 0.20238462 0.049     ]\n"
     ]
    }
   ],
   "source": [
    "result = permutation_importance(hgb, test_X, test_y,\n",
    "n_repeats = 10 ,random_state = 42, n_jobs = -1)\n",
    "\n",
    "print(result.importances_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8b752fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8723076923076923"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgb.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625cd63c",
   "metadata": {},
   "source": [
    "We can see a 87% accuracy. \n",
    "\n",
    "Of course, in real life, the performance will be a bit low than this. \n",
    "\n",
    "However still, ensemble learning can acheive a higher score than single decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3416fa20",
   "metadata": {},
   "source": [
    "# RandomForest vs ExtraTrees vs GradientBoosting vs HistGradientBoosting\n",
    "\n",
    "## RandomForestClassifier\n",
    "- Builds many decision trees using bootstrap samples and random feature subsets  \n",
    "- Parallel, stable, low overfitting risk  \n",
    "- Strong baseline, works well with noisy data\n",
    "\n",
    "## ExtraTreesClassifier\n",
    "- Similar to RandomForest but splits are chosen completely at random  \n",
    "- Faster training, more variance reduction  \n",
    "- Often slightly less accurate but more efficient\n",
    "\n",
    "## GradientBoostingClassifier\n",
    "- Sequentially builds trees that correct previous errors  \n",
    "- High accuracy potential but slower and more prone to overfitting  \n",
    "- Requires tuning (learning_rate, depth)\n",
    "\n",
    "## HistGradientBoostingClassifier\n",
    "- Histogram-based version of Gradient Boosting  \n",
    "- Much faster on large datasets  \n",
    "- Handles missing values natively and scales better than standard GradientBoosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ade677",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0359afae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9567059184812372 0.8783915747390243\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(tree_method='hist', random_state=42)\n",
    "xgb._estimator_type = \"classifier\"  # ensure compatibility with sklearn utilities\n",
    "\n",
    "scores = cross_validate(\n",
    "    xgb, train_X, train_y,\n",
    "    return_train_score=True, n_jobs=-1\n",
    ")\n",
    "\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30350a73",
   "metadata": {},
   "source": [
    "# LightGBM\n",
    "\n",
    "LightGBM is also a fast gradient boosting implementation, developed by Microsoft. It also implements lots of updated technologies, which is making it more popular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0d2b2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 3151, number of negative: 1006[LightGBM] [Info] Number of positive: 3151, number of negative: 1007\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 3151, number of negative: 1007\n",
      "[LightGBM] [Info] Number of positive: 3152, number of negative: 1006\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000895 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 372\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000829 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000862 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000865 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 371\n",
      "[LightGBM] [Info] Total Bins 373\n",
      "[LightGBM] [Info] Total Bins 372\n",
      "[LightGBM] [Info] Number of data points in the train set: 4157, number of used features: 3\n",
      "[LightGBM] [Info] Number of data points in the train set: 4158, number of used features: 3\n",
      "[LightGBM] [Info] Number of data points in the train set: 4158, number of used features: 3\n",
      "[LightGBM] [Info] Number of data points in the train set: 4158, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.757816 -> initscore=1.140744[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.757816 -> initscore=1.140744\n",
      "\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.757999 -> initscore=1.141738\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.758057 -> initscore=1.142055\n",
      "[LightGBM] [Info] Start training from score 1.140744\n",
      "[LightGBM] [Info] Start training from score 1.140744\n",
      "[LightGBM] [Info] Start training from score 1.142055\n",
      "[LightGBM] [Info] Start training from score 1.141738\n",
      "[LightGBM] [Info] Number of positive: 3151, number of negative: 1006\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001094 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 370\n",
      "[LightGBM] [Info] Number of data points in the train set: 4157, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.757999 -> initscore=1.141738\n",
      "[LightGBM] [Info] Start training from score 1.141738\n",
      "0.935828414851749 0.8801251203079884\n"
     ]
    }
   ],
   "source": [
    "lgb = LGBMClassifier(random_state=42)\n",
    "scores = cross_validate(\n",
    "    lgb, train_X, train_y,\n",
    "    return_train_score=True, n_jobs=-1\n",
    ")\n",
    "\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1792ffe",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "- Optimized gradient boosting with regularization, tree pruning, and efficient handling of sparse data  \n",
    "- Very strong performance, widely used in competitions and structured data tasks\n",
    "\n",
    "## LightGBM\n",
    "- Gradient boosting using histogram and leaf-wise growth for faster training and lower memory use  \n",
    "- Scales extremely well on large datasets and often outperforms other boosting methods in speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d397332",
   "metadata": {},
   "source": [
    "# What I learned\n",
    "\n",
    "On this wine dataset the tree ensembles form a clear pattern. Random Forest and Extra Trees achieve high training accuracy and strong test accuracy without scaling. Gradient Boosting, Histogram based Gradient Boosting, XGBoost, and LightGBM sit in a similar performance band while offering different speed and tuning behavior. Sugar is consistently the most informative feature which aligns with the decision trees from earlier chapters. For a quick and reliable baseline I can start with Random Forest. When I need a bit more headroom or more control I can switch to a boosting model and tune the number of trees and the learning rate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
